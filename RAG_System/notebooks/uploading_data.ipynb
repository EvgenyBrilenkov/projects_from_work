{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как создать свою БД:  \n",
    "- Скачайте PostgreSQL и pgvector  \n",
    "- С помощью команд ниже подключите расширение vector и создайте таблицу documents_e5\n",
    "\n",
    "PostgreSQL: (в cmd)  \n",
    "psql -h localhost -d appdb -U appuser  \n",
    "CREATE EXTENSION IF NOT EXISTS vector;  \n",
    "CREATE TABLE documents_e5 (id SERIAL PRIMARY KEY, doc_id TEXT, chunk_id INT, content TEXT, embedding vector(1024), metadata JSONB);  \n",
    "\n",
    "Также создайте таблицу chat_history для будущего логирования запросов.  \n",
    "CREATE TABLE chat_history (id SERIAL PRIMARY KEY, user_id TEXT, doc_id TEXT, user_message TEXT, rephrased_message TEXT, assistant_message TEXT, timestamp TIMESTAMP, sources_ids TEXT); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "FOLDER_PATH = \"/wrk/data/raw/Документы\" # Путь до папки с документами \n",
    "DB_CONN = \"dbname=appdb user=appuser password=secret port=5432 host=rag-data\" # Подключение к созданной БД\n",
    "MODEL_PATH = \"/wrk/models/embedding_models/models--intfloat--multilingual-e5-large-instruct/snapshots/274baa43b0e13e37fafa6428dbc7938e62e5c439\" # Путь до вашей эмбеддинговой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rag_sys/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaModel(\n",
       "  (embeddings): XLMRobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): XLMRobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x XLMRobertaLayer(\n",
       "        (attention): XLMRobertaAttention(\n",
       "          (self): XLMRobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): XLMRobertaSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): XLMRobertaIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): XLMRobertaOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): XLMRobertaPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model uploading\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModel.from_pretrained(MODEL_PATH)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading, parsing and creating embeddings functions\n",
    "import pdfplumber\n",
    "from docx import Document\n",
    "import torch.nn.functional as F\n",
    "import textract\n",
    "import re\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "CHUNK_SIZE = 1500 # Количество символов для одного чанка (кусочка документа)\n",
    "OVERLAP = 200 # Количество символов для перекрытия чанков (чтобы модель видела контекст)\n",
    "\n",
    "# Ignoring useless tokens\n",
    "def average_pool(last_hidden_states, attention_mask):\n",
    "    mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
    "    sum_embeddings = torch.sum(last_hidden_states * mask_expanded, 1)\n",
    "    sum_mask = mask_expanded.sum(1).clamp(min=1e-9)\n",
    "    return sum_embeddings/sum_mask\n",
    "    \n",
    "# Creating embeddings from text\n",
    "def embed(text: str):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "        ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        emb = average_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
    "        emb = F.normalize(emb, p=2, dim=1)\n",
    "    return emb[0].cpu().numpy()\n",
    "\n",
    "# Getting text from pdf\n",
    "def load_pdf(path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Getting text from docx\n",
    "def load_docx(path):\n",
    "    doc = Document(path)\n",
    "    return \"\\n\".join([row.text for row in doc.paragraphs if row.text.strip()])\n",
    "\n",
    "# Getting text from doc\n",
    "def load_doc(path):\n",
    "    text = textract.process(path).decode(\"utf-8\")\n",
    "    return text.strip()\n",
    "\n",
    "# Cleaning text\n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[-=*]{3,}', ' ', text)\n",
    "    text = re.sub(r'([.,!?;:])([^\\s])', r'\\1 \\2', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Chunking extracted text\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=OVERLAP):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data in PostgreSQL function\n",
    "import psycopg2\n",
    "import json\n",
    "\n",
    "conn = psycopg2.connect(DB_CONN)\n",
    "cur = conn.cursor()\n",
    "\n",
    "def save_chunk(doc_id, chunk_id, text, metadata = {}):\n",
    "    emb = embed(text)\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO documents_e5 (doc_id, chunk_id, content, embedding, metadata) VALUES (%s, %s, %s, %s, %s)\n",
    "        \"\"\",\n",
    "        (doc_id, chunk_id, text, emb.tolist(), json.dumps(metadata))\n",
    "    )\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go!\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def load_docs(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        path = os.path.join(folder_path, filename)\n",
    "        ext = os.path.splitext(filename)[1].lower()\n",
    "        \n",
    "        if ext == \".pdf\":\n",
    "            text = load_pdf(path)\n",
    "        elif ext == \".docx\":\n",
    "            text = load_docx(path)\n",
    "        elif ext == \".doc\":\n",
    "            text = load_doc(path)\n",
    "        else:\n",
    "            print(f\"Формат файла {filename} не поддерживается.\")\n",
    "            continue\n",
    "        \n",
    "        text = clean_text(text)\n",
    "        \n",
    "        chunks = chunk_text(text)\n",
    "    \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            save_chunk(\n",
    "                doc_id=filename,\n",
    "                chunk_id=i,\n",
    "                text=chunk,\n",
    "                metadata={\"путь\": path, \"дата_последнего_изменения\": datetime.fromtimestamp(os.path.getmtime(path)).strftime(\"%d.%m.%Y\")}\n",
    "            )\n",
    "        \n",
    "        print(f\"{filename}: {len(chunks)} чанков сохранено\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_docs(FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После загрузки документов проиндексируйте чанки с помощью команды ниже.  \n",
    "В случае, если вы будете добавлять новые документы, не забывайте сбрасывать индексы и создавать их заново (для индексации всех докуметов)  \n",
    "\n",
    "CREATE INDEX ON documents_e5 USING ivfflat (embedding vector_cosine_ops);  \n",
    "    --NOTICE:  ivfflat index created with little data  \n",
    "    --DETAIL:  This will cause low recall.  \n",
    "    --HINT:  Drop the index until the table has more data.  \n",
    "    (DROP INDEX IF EXISTS name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_sys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
