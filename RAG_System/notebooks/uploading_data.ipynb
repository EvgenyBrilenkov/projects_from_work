{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как создать свою БД:  \n",
    "- Скачайте PostgreSQL и pgvector  \n",
    "- С помощью команд ниже подключите расширение vector и создайте таблицу documents\n",
    "\n",
    "PostgreSQL: (в cmd)  \n",
    "psql -h localhost -d appdb -U appuser  \n",
    "CREATE EXTENSION IF NOT EXISTS vector;  \n",
    "CREATE TABLE documents (id SERIAL PRIMARY KEY, doc_id TEXT, chunk_id INT, content TEXT, embedding vector(1024), metadata JSONB);  \n",
    "\n",
    "Также создайте таблицу chat_history для будущего логирования запросов.  \n",
    "CREATE TABLE chat_history (id SERIAL PRIMARY KEY, user_id TEXT, doc_id TEXT, user_message TEXT, rephrased_message TEXT, assistant_message TEXT, timestamp TIMESTAMP, sources_ids TEXT, chunks TEXT); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "FOLDER_PATH = \"/wrk/data\"\n",
    "DB_CONN = \"dbname=appdb user=appuser password=secret port=5433 host=10.101.10.106\"\n",
    "MODEL_PATH = \"/wrk/models/models--intfloat--multilingual-e5-large-instruct/snapshots/274baa43b0e13e37fafa6428dbc7938e62e5c439\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/rag-lite/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaModel(\n",
       "  (embeddings): XLMRobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): XLMRobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x XLMRobertaLayer(\n",
       "        (attention): XLMRobertaAttention(\n",
       "          (self): XLMRobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): XLMRobertaSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): XLMRobertaIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): XLMRobertaOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): XLMRobertaPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model uploading\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "emb_tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "emb_model = AutoModel.from_pretrained(MODEL_PATH)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "emb_model = emb_model.to(device)\n",
    "emb_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading, parsing and creating embeddings functions\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, TesseractCliOcrOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import granite_picture_description\n",
    "from docling.chunking import HybridChunker\n",
    "from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "\n",
    "# Text extractor config\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.do_ocr = False\n",
    "pipeline_options.do_table_structure = True\n",
    "pipeline_options.table_structure_options.do_cell_matching = True\n",
    "# ocr_options = TesseractCliOcrOptions(force_full_page_ocr=True, lang = ['rus'])\n",
    "# pipeline_options.ocr_options = ocr_options\n",
    "pipeline_options.do_formula_enrichment = True\n",
    "pipeline_options.do_picture_description = True\n",
    "pipeline_options.picture_description_options = granite_picture_description\n",
    "pipeline_options.picture_description_options.prompt = (\"Describe the picture.\")\n",
    "pipeline_options.images_scale = 1\n",
    "pipeline_options.generate_picture_images = True\n",
    "\n",
    "converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "        pipeline_options=pipeline_options)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Chunking config\n",
    "MAX_LENGTH = 512\n",
    "CHUNK_SIZE = 1500\n",
    "OVERLAP = 200\n",
    "\n",
    "# Ignoring useless tokens\n",
    "def average_pool(last_hidden_states, attention_mask):\n",
    "    mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
    "    sum_embeddings = torch.sum(last_hidden_states * mask_expanded, 1)\n",
    "    sum_mask = mask_expanded.sum(1).clamp(min=1e-9)\n",
    "    return sum_embeddings/sum_mask\n",
    "    \n",
    "# Creating embeddings from text\n",
    "def embed(text: str):\n",
    "    inputs = emb_tokenizer(\n",
    "        \"passage: \"+text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "        ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = emb_model(**inputs)\n",
    "        emb = average_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
    "        emb = F.normalize(emb, p=2, dim=1)\n",
    "    return emb[0].cpu().numpy()\n",
    "\n",
    "# Getting text from pdf\n",
    "def load_doc(path):\n",
    "    doc = converter.convert(path)\n",
    "    return doc.document\n",
    "\n",
    "# Chunking extracted text\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=OVERLAP):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data in PostgreSQL function\n",
    "import psycopg2\n",
    "import json\n",
    "\n",
    "conn = psycopg2.connect(DB_CONN)\n",
    "cur = conn.cursor()\n",
    "\n",
    "def save_chunk(doc_id, chunk_id, text, metadata = {}):\n",
    "    emb = embed(text)\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO documents (doc_id, chunk_id, content, embedding, metadata) VALUES (%s, %s, %s, %s, %s)\n",
    "        \"\"\",\n",
    "        (doc_id, chunk_id, text, emb.tolist(), json.dumps(metadata))\n",
    "    )\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go!\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def load_docs(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        path = os.path.join(folder_path, filename)\n",
    "\n",
    "        try:\n",
    "            doc = load_doc(path)\n",
    "        except:\n",
    "            print(f\"Формат файла {filename} не поддерживается.\")\n",
    "            continue\n",
    "        \n",
    "        chunks = chunk_text(doc.export_to_text())\n",
    "        name = f\"\"\"Отрывок из документа: \"{filename}\"\n",
    "\n",
    "\"\"\"\n",
    "        path = \"/wrk/data/База данных pdf/\"+filename\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            save_chunk(\n",
    "                doc_id=filename,\n",
    "                chunk_id=i,\n",
    "                text=name+chunk,\n",
    "                metadata={\"путь\": path, \"дата_последнего_изменения\": datetime.fromtimestamp(os.path.getmtime(path)).strftime(\"%d.%m.%Y\")}\n",
    "            )\n",
    "        \n",
    "        print(f\"{filename}: {len(chunks)} чанков сохранено\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если в папке лежат документы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_docs(FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если в папке есть другие папки с документами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for document in os.listdir(FOLDER_PATH):\n",
    "#     main_path = os.path.join(FOLDER_PATH, document)\n",
    "#     load_docs(main_path)#FOLDER_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После загрузки документов проиндексируйте чанки с помощью команды ниже.  \n",
    "В случае, если вы будете добавлять новые документы, не забывайте сбрасывать индексы и создавать их заново (для индексации всех докуметов)  \n",
    "\n",
    "CREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops);  \n",
    "    --NOTICE:  ivfflat index created with little data  \n",
    "    --DETAIL:  This will cause low recall.  \n",
    "    --HINT:  Drop the index until the table has more data.  \n",
    "    (DROP INDEX IF EXISTS name)\n",
    "\n",
    "Затем необходимо добавить колонку и индексы для алгоритма BM25:  \n",
    "ALTER TABLE documents ADD COLUMN tsv tsvector;  \n",
    "UPDATE documents SET tsv = to_tsvector('russian', content);  \n",
    "CREATE INDEX idx_documents_tsv ON documents USING gin(tsv);  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь необходимо создать таблицы для будущего data layer приложения\n",
    "\n",
    "CREATE TABLE users (\n",
    "    \"id\" UUID PRIMARY KEY,\n",
    "    \"identifier\" TEXT NOT NULL UNIQUE,\n",
    "    \"metadata\" JSONB NOT NULL,\n",
    "    \"createdAt\" TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS threads (\n",
    "    \"id\" UUID PRIMARY KEY,\n",
    "    \"createdAt\" TEXT,\n",
    "    \"name\" TEXT,\n",
    "    \"userId\" UUID,\n",
    "    \"userIdentifier\" TEXT,\n",
    "    \"tags\" TEXT[],\n",
    "    \"metadata\" JSONB,\n",
    "    FOREIGN KEY (\"userId\") REFERENCES users(\"id\") ON DELETE CASCADE\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS steps (\n",
    "    \"id\" UUID PRIMARY KEY,\n",
    "    \"name\" TEXT NOT NULL,\n",
    "    \"type\" TEXT NOT NULL,\n",
    "    \"threadId\" UUID NOT NULL,\n",
    "    \"parentId\" UUID,\n",
    "    \"streaming\" BOOLEAN NOT NULL,\n",
    "    \"waitForAnswer\" BOOLEAN,\n",
    "    \"isError\" BOOLEAN,\n",
    "    \"metadata\" JSONB,\n",
    "    \"tags\" TEXT[],\n",
    "    \"input\" TEXT,\n",
    "    \"output\" TEXT,\n",
    "    \"createdAt\" TEXT,\n",
    "    \"command\" TEXT,\n",
    "    \"start\" TEXT,\n",
    "    \"end\" TEXT,\n",
    "    \"generation\" JSONB,\n",
    "    \"showInput\" TEXT,\n",
    "    \"language\" TEXT,\n",
    "    \"indent\" INT,\n",
    "    \"defaultOpen\" BOOLEAN,\n",
    "    FOREIGN KEY (\"threadId\") REFERENCES threads(\"id\") ON DELETE CASCADE\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS elements (\n",
    "    \"id\" UUID PRIMARY KEY,\n",
    "    \"threadId\" UUID,\n",
    "    \"type\" TEXT,\n",
    "    \"url\" TEXT,\n",
    "    \"chainlitKey\" TEXT,\n",
    "    \"name\" TEXT NOT NULL,\n",
    "    \"display\" TEXT,\n",
    "    \"objectKey\" TEXT,\n",
    "    \"size\" TEXT,\n",
    "    \"page\" INT,\n",
    "    \"language\" TEXT,\n",
    "    \"forId\" UUID,\n",
    "    \"mime\" TEXT,\n",
    "    \"props\" JSONB,\n",
    "    FOREIGN KEY (\"threadId\") REFERENCES threads(\"id\") ON DELETE CASCADE\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS feedbacks (\n",
    "    \"id\" UUID PRIMARY KEY,\n",
    "    \"forId\" UUID NOT NULL,\n",
    "    \"threadId\" UUID NOT NULL,\n",
    "    \"value\" INT NOT NULL,\n",
    "    \"comment\" TEXT,\n",
    "    FOREIGN KEY (\"threadId\") REFERENCES threads(\"id\") ON DELETE CASCADE\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылка на таблицу: postgresql+psycopg2://appuser:secret@10.101.10.106:5433/appdb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-lite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
