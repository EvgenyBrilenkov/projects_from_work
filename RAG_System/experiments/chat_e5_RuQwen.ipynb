{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "DB_CONN = \"dbname=appdb user=appuser password=secret port=5432 host=rag-data\"\n",
    "EMB_MODEL_PATH = \"/wrk/models/embedding_models/models--intfloat--multilingual-e5-large-instruct/snapshots/274baa43b0e13e37fafa6428dbc7938e62e5c439\"\n",
    "LLM_MODEL_PATH = \"/wrk/models/llms/models--RefalMachine--RuadaptQwen2.5-7B-Lite-Beta-GGUF/snapshots/68ae9dff37a839f3441b9383519cffc4f7d829dd/FP16.gguf\"\n",
    "TOP_K = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "print(hasattr(Llama, \"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /wrk/models/llms/models--RefalMachine--RuadaptQwen2.5-7B-Lite-Beta-GGUF/snapshots/68ae9dff37a839f3441b9383519cffc4f7d829dd/FP16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = RuadaptQwen2.5 7B Lite Beta\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Lite-Beta\n",
      "llama_model_loader: - kv   4:                           general.basename str              = RuadaptQwen2.5\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   6:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv   7:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 3584\n",
      "llama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 18944\n",
      "llama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 28\n",
      "llama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,145152]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,145152]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,144853]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 145109\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 145111\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 145109\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type  f16:  198 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = F16\n",
      "print_info: file size   = 14.09 GiB (16.00 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 145149 '<|free_token19|>' is not marked as EOG\n",
      "load: control token: 145146 '<|free_token16|>' is not marked as EOG\n",
      "load: control token: 145144 '<|free_token14|>' is not marked as EOG\n",
      "load: control token: 145142 '<|free_token12|>' is not marked as EOG\n",
      "load: control token: 145139 '<|free_token9|>' is not marked as EOG\n",
      "load: control token: 145138 '<|free_token8|>' is not marked as EOG\n",
      "load: control token: 145134 '<|free_token4|>' is not marked as EOG\n",
      "load: control token: 145132 '<|free_token2|>' is not marked as EOG\n",
      "load: control token: 145131 '<|free_token1|>' is not marked as EOG\n",
      "load: control token: 145126 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 145125 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 145121 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 145119 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 145117 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 145115 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 145114 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 145112 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 145151 '<|free_token21|>' is not marked as EOG\n",
      "load: control token: 145124 '</tool_call>' is not marked as EOG\n",
      "load: control token: 145133 '<|free_token3|>' is not marked as EOG\n",
      "load: control token: 145148 '<|free_token18|>' is not marked as EOG\n",
      "load: control token: 145136 '<|free_token6|>' is not marked as EOG\n",
      "load: control token: 145113 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 145145 '<|free_token15|>' is not marked as EOG\n",
      "load: control token: 145140 '<|free_token10|>' is not marked as EOG\n",
      "load: control token: 145118 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 145150 '<|free_token20|>' is not marked as EOG\n",
      "load: control token: 145120 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 145123 '<tool_call>' is not marked as EOG\n",
      "load: control token: 145147 '<|free_token17|>' is not marked as EOG\n",
      "load: control token: 145122 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 145110 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 145135 '<|free_token5|>' is not marked as EOG\n",
      "load: control token: 145127 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 145143 '<|free_token13|>' is not marked as EOG\n",
      "load: control token: 145116 '<|quad_start|>' is not marked as EOG\n",
      "load: control token: 145137 '<|free_token7|>' is not marked as EOG\n",
      "load: control token: 145141 '<|free_token11|>' is not marked as EOG\n",
      "load: printing all EOG tokens:\n",
      "load:   - 145109 ('<|endoftext|>')\n",
      "load:   - 145111 ('<|im_end|>')\n",
      "load:   - 145128 ('<|fim_pad|>')\n",
      "load:   - 145129 ('<|repo_name|>')\n",
      "load:   - 145130 ('<|file_sep|>')\n",
      "load: special tokens cache size = 43\n",
      "load: token to piece cache size = 1.2225 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 3584\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 28\n",
      "print_info: n_head_kv        = 4\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 7\n",
      "print_info: n_embd_k_gqa     = 512\n",
      "print_info: n_embd_v_gqa     = 512\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 18944\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = -1\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.57 B\n",
      "print_info: general.name     = RuadaptQwen2.5 7B Lite Beta\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 145152\n",
      "print_info: n_merges         = 144853\n",
      "print_info: BOS token        = 145109 '<|endoftext|>'\n",
      "print_info: EOS token        = 145111 '<|im_end|>'\n",
      "print_info: EOT token        = 145109 '<|endoftext|>'\n",
      "print_info: PAD token        = 145109 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 145125 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 145127 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 145126 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 145128 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 145129 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 145130 '<|file_sep|>'\n",
      "print_info: EOG token        = 145109 '<|endoftext|>'\n",
      "print_info: EOG token        = 145111 '<|im_end|>'\n",
      "print_info: EOG token        = 145128 '<|fim_pad|>'\n",
      "print_info: EOG token        = 145129 '<|repo_name|>'\n",
      "print_info: EOG token        = 145130 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (f16) (and 338 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size = 14431.77 MiB\n",
      ".........................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 3000\n",
      "llama_context: n_ctx_per_seq = 3000\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (3000) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.55 MiB\n",
      "create_memory: n_ctx = 3008 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   164.50 MiB\n",
      "llama_kv_cache_unified: size =  164.50 MiB (  3008 cells,  28 layers,  1/1 seqs), K (f16):   82.25 MiB, V (f16):   82.25 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 2\n",
      "llama_context: max_nodes = 2712\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   290.50 MiB\n",
      "llama_context: graph nodes  = 1070\n",
      "llama_context: graph splits = 394 (with bs=512), 1 (with bs=1)\n",
      "CPU : SSE3 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.bos_token_id': '145109', 'general.file_type': '1', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '145109', 'general.basename': 'RuadaptQwen2.5', 'qwen2.embedding_length': '3584', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'RuadaptQwen2.5 7B Lite Beta', 'qwen2.block_count': '28', 'general.finetune': 'Lite-Beta', 'general.type': 'model', 'general.size_label': '7B', 'qwen2.context_length': '32768', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'qwen2.attention.head_count_kv': '4', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '18944', 'qwen2.attention.head_count': '28', 'tokenizer.ggml.eos_token_id': '145111', 'qwen2.rope.freq_base': '1000000.000000'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Models uploading\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from llama_cpp import Llama\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "emb_tokenizer = AutoTokenizer.from_pretrained(EMB_MODEL_PATH)\n",
    "emb_model = AutoModel.from_pretrained(EMB_MODEL_PATH)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "emb_model = emb_model.to(device)\n",
    "emb_model.eval()\n",
    "\n",
    "\n",
    "llm_model = Llama(\n",
    "    model_path=LLM_MODEL_PATH,\n",
    "    main_gpu=0,\n",
    "    n_ctx=3000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating embeddings function\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# Ignoring useless tokens\n",
    "def average_pool(last_hidden_states, attention_mask):\n",
    "    mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
    "    sum_embeddings = torch.sum(last_hidden_states * mask_expanded, 1)\n",
    "    sum_mask = mask_expanded.sum(1).clamp(min=1e-9)\n",
    "    return sum_embeddings/sum_mask\n",
    "\n",
    "# Creating embeddings from text\n",
    "def embed(text: str):\n",
    "    inputs = emb_tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "        ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = emb_model(**inputs)\n",
    "        emb = average_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
    "        emb = F.normalize(emb, p=2, dim=1)\n",
    "    return emb[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching relevant documents\n",
    "import psycopg2\n",
    "import json\n",
    "\n",
    "conn = psycopg2.connect(DB_CONN)\n",
    "cur = conn.cursor()\n",
    "\n",
    "def search_context(query, top_k=TOP_K):\n",
    "    query_emb = embed(query).tolist()\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        SELECT content, metadata FROM documents_e5 ORDER BY embedding <-> %s LIMIT %s\n",
    "        \"\"\",\n",
    "        (json.dumps(query_emb), top_k)\n",
    "    )\n",
    "    results = cur.fetchall()\n",
    "    return [r[0] for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_context(\"Вопрос\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asking LLM\n",
    "def ask_llm(question, context):\n",
    "    prompt = f\"\"\"Ты - умный ассистент, помогающий сотрудникам ответить на вопросы. Используй приведённый контекст для ответа на вопросы.\n",
    "    \n",
    "    \n",
    "    Контекст:\n",
    "    {context}\n",
    "    \n",
    "    \n",
    "    Вопрос: {question}\n",
    "    Ответ:\"\"\"\n",
    "    \n",
    "    return llm_model(\n",
    "        prompt,\n",
    "        max_tokens=3500,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9)['choices'][0]['text'].strip()\n",
    "\n",
    "def answer_question(question: str):\n",
    "    context_chunks = search_context(question)\n",
    "    context = \"\\n\\n\".join(context_chunks)\n",
    "    return ask_llm(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"Вопрос\"\n",
    "print(answer_question(q))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_sys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
